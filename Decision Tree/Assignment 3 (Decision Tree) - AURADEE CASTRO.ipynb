{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assignment 3 (Decision Tree)**\n",
    "**Student Name**: Auradee Castro | **Student Number**: c0866821\n",
    "\n",
    "**Github Link**: https://github.com/abccastro/Simple-ML-Coding/blob/main/Decision%20Tree/Assignment%203%20(Decision%20Tree)%20-%20AURADEE%20CASTRO.ipynb\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q1. Describe the decision tree classifier algorithm and how it works to make predictions.__\n",
    "\n",
    "The Decision Tree classifier classifier algorithm uses a tree structure to predict outcomes based on supervised learning. It's trained on pre-processed data, following a top-down approach, meaning that the root node of the tree is always at the top of the structure while the outcomes are represented by the tree leaves. Recursive partitioning (commonly referred to as Divide and Conquer) is used to split nodes, creating a tree that partitions the data into dense and sparse regions. This process continues until the data is homogeneous. The resulting tree allows for accurate categorized predictions. Each split creates a decision node, and leaves represent predictions. To make a final prediction, follow the path from the root to a leaf and average the target values of the instances at that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.__\n",
    "\n",
    "__Information Gain__\n",
    "\n",
    "Information gain (IG) measures the reduction in entropy or variance that occurs when a dataset is divided based on a specific property. It is used in decision tree algorithms to determine the importance of a feature by creating subsets that are more homogenous with respect to class labels or the target variable. A greater information gain indicates that a feature is more valuable in predicting the target variable. Building a decision tree revolves around identifying an attribute that yields the highest information gain and the lowest entropy.\n",
    "\n",
    "__Entropy__\n",
    "\n",
    "Entropy is a measure of the randomness in the information being processed. In the case of classifications, it measures the randomness based on the distribution of class labels in the dataset. The higher the entropy, the harder it is to draw any conclusions from that information.\n",
    "\n",
    "Key points regarding entropy:\n",
    "\n",
    "1. Entropy reaches its minimum value of 0 when all instances belong to the same class, indicating low uncertainty in the dataset.\n",
    "2. Maximum entropy occurs when class labels are evenly distributed, signifying high uncertainty.\n",
    "3. Entropy evaluates split effectiveness by identifying the attribute that minimizes subset entropy through creating more uniform subsets in terms of class labels.\n",
    "4. Attribute with highest information gain is selected as the criterion for splitting, which guides tree construction through iterative splitting.\n",
    "\n",
    "__Gini Impurity or Index__\n",
    "\n",
    "Gini index is a measure of impurity or purity used while creating a decision tree algorithm. It serves as a metric for evaluating the effectiveness of a split in classifying groups. It provides a score between 0 and 1, where 0 indicates that all observations belong to a single class, and 1 represents a random distribution of elements across classes. Higher value of Gini index implies higher inequality, higher heterogeneity. In this context, the goal is to minimize the Gini index score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.__\n",
    "\n",
    "A decision tree classifier uses a hierarchical set of rules based on features to make binary classifications. It works by recursively partitioning the feature space into subsets based on the values of input features. It starts with the entire dataset and chooses the feature that best splits the data into two subsets with the highest information gain or Gini impurity reduction. This process is repeated until a stopping criterion is met, such as a maximum depth or a minimum number of samples per leaf. \n",
    "\n",
    "In a binary classification problem, the goal is to predict one of two possible outcomes based on the input features. For instance, the decision tree would use features like age, state of residence, income, and political leaning to make a prediction about whether the person is male or female. The model employs a series of if-else questions based on feature thresholds to navigate through the feature space and classify the data points into one of the two classes. This process creates a tree-like structure where each node represents a decision point, and each leaf node represents a class prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.__\n",
    "\n",
    "The process of constructing a decision tree involves recursively partitioning the feature space based on dataset attributes. Here's a step-by-step overview of how the algorithm learns and creates the tree.\n",
    "\n",
    "1. **Feature Selection**: Beginning at the root node, the algorithm identifies the best feature for partitioning the data. This choice is determined by a criterion that aims to maximize information gain or minimize impurity.\n",
    "\n",
    "2. **Data Partitioning**: The dataset is split into subsets based on the values of the chosen feature. Each subset corresponds to a branch originating from the current node.\n",
    "\n",
    "3. **Repetition of the Process**: This process is iterated for each subset, treating them as distinct subproblems. The algorithm identifies the optimal feature to partition each subset and creates child nodes accordingly.\n",
    "\n",
    "    <em> __NOTE__: Steps 2 and 3 is also called \"Recursive Partitioning\". It is repeatedly dividing and subdividing the data with the goal of making the outcomes in each final subdivision as homogeneous as possible.</em>\n",
    "\n",
    "4. **Stopping Conditions**: The procedure continues until a stopping condition is met. Common stopping conditions may include reaching a maximum depth, ensuring a minimum number of samples in a node, or achieving a pure class distribution.\n",
    "\n",
    "5. **Leaf Node Formation**: When the stopping conditions are satisfied, the algorithm generates leaf nodes with class labels for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.__\n",
    "\n",
    "A __confusion matrix__ is a square matrix with dimensions N x N, which used to evaluate the performance of classification model. N represents the total number of classes the model is trying to predict. The matrix allows for a comparison between the actual values and the predictions made by the model. It serves as a tool to measure the effectiveness of the classification model, enabling the computation of metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "<img title=\"Confusion Matrix\" alt=\"Confusion Matrix\" src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z54JgbS4DUwWSknhDCvNTQ.png\">\n",
    "\n",
    "Below are the four basic terminology to help us in determining the metrics:\n",
    "\n",
    "1. __True Positive (TP)__: the model correctly predicted the positive class\n",
    "2. __True Negative (TN)__: the model correctly predicted the negative class\n",
    "3. __False Positive (FP)__: the model incorrectly predicted the positive class when it was actually negative; a.k.a Type I error\n",
    "4. __False Negative (FN)__: the model incorrectly predicted the negative class when it was actually positive; a.k.a Type II error\n",
    "\n",
    "\n",
    "Below are several performance metrics that can be derived from confusion matrix to assess the model's performance.\n",
    "\n",
    "1. __Accuracy__: ratio of correctly classified instances out of the total number of instances\n",
    "\n",
    "$$ Accuracy = {TP + TN \\over TP + TN + FP + FN} $$\n",
    "\n",
    "2. __Precision__: ratio of true positive predictions out of all positive predictions (how many of the predicted positive classes were actually correct)\n",
    "\n",
    "$$ Precision = {TP \\over TP + FP} $$\n",
    "\n",
    "3. __Recall__: ratio of true positive predictions out of all actual positive instances (ability of the model to correctly identify all positive instances)\n",
    "\n",
    "$$ Recall = {TP \\over TP + FN} $$\n",
    "\n",
    "4. __F1 Score__: harmonic mean of precision and recall (provides balance between precision and recall)\n",
    "\n",
    "$$ F1-score = {2 * Precision * Recall \\over Precision + Recall} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.__\n",
    "\n",
    "Supposed there's a dataset of 100 emails. The model predictions are as follows.\n",
    "- True Positives (TP): 30 emails correctly predicted as spam\n",
    "- False Positives (FP): 10 emails incorrectly predicted as spam when they were actually not spam\n",
    "- True Negatives (TN): 50 emails correctly predicted as not spam\n",
    "- False Negatives (FN): 10 emails incorrectly predicted as not spam when they were actually spam\n",
    "\n",
    "\n",
    "|                           |    Actual Spam (+)    |   Actual Not Spam (-)  |\n",
    "| ------------------------- | --------------------- | ---------------------- |\n",
    "| Predicted Spam (+)        |   30 (TP)             |   20 (FP)              |\n",
    "| Predicted Not Spam (-)    |   10 (FN)             |   40 (TN)              |\n",
    "\n",
    "\n",
    "Calculate precision, recall, and F1-score based on confusion matrix\n",
    "1. Precision = TP / (TP + FP) = 30 / (30 + 10) = __0.60__\n",
    "2. Recall = TP / (TP + FN) = 30 / (30 + 10) = __0.75__\n",
    "3. F1 Score = (2 * Precision * Recall) / (Precision + Recall) = (2 * 0.60 * 0.75) / (0.60 + 0.75) = __0.667__\n",
    "\n",
    "A recall of 0.75 indicates that the model correctly identified 75% of all actual positive instances. A precision of 0.60 means that out of all the instances predicted as positive, only 60% were actually true positives. This suggests that the model is relatively better at identifying actual positives but has a higher rate of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.__\n",
    "\n",
    "Choosing an appropriate evaluation metric is important in machine learning, as it determines how the performance of a model is evaluated and can greatly impact decision-making in real-world applications. The choice of metric should align with the specific goals and requirements of the problem.\n",
    "\n",
    "1. **Reflecting Business Objectives**: Different classification problems have different priorities. For example, in medical diagnosis, minimizing false negatives (Type II error) may be critical to ensure no actual cases are missed. In fraud detection, minimizing false positives (Type I error) is often more important to avoid inconveniencing legitimate customers.\n",
    "2. **Dealing with Imbalanced Data**: In cases where one class is much more prevalent than the other (class imbalance), metrics like accuracy can be misleading. Choosing metrics like precision, recall, or F1-score can provide a more balanced view of performance.\n",
    "\n",
    "3. **Understanding Costs of Errors**: Consider the costs associated with false positives and false negatives. For example, in a medical context, the cost of missing a disease may be higher than performing additional tests. Understanding these costs helps in selecting an appropriate metric.\n",
    "\n",
    "4. **Balancing Trade-offs**: Some metrics, like the F1-score, strike a balance between precision and recall. This can be useful when both types of errors (false positives and false negatives) have different implications.\n",
    "\n",
    "5. **Domain Expertise and Stakeholder Input**: Consulting with domain experts and stakeholders can provide valuable insights into what metrics are most relevant and critical for the problem at hand. They can offer valuable perspectives on the potential impact of different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q8. Provide an example of a classification problem where precision is the most important metric, and explain why.__\n",
    "\n",
    "Consider a scenario in a financial institution where a model is used to detect fraudulent transactions:\n",
    "\n",
    "- **Positive Class (1)**: Transaction is fraudulent\n",
    "- **Negative Class (0)**: Transaction is legitimate\n",
    "\n",
    "The model generates the following performance metrics:\n",
    "\n",
    "- **Precision**: 98%\n",
    "- **Recall**: 85%\n",
    "\n",
    "Analysis on the importance of precision:\n",
    "\n",
    "1. **False Positives (Type I Error)**: The model incorrectly predicts a legitimate transaction as fraudulent. While this may provide inconvenience to a customer temporarily (e.g., their card may be temporarily blocked), it's generally preferable to err on the side of caution and investigate potential fraud.\n",
    "\n",
    "2. **False Negatives (Type II Error)**: The model fails to detect an actual fraudulent transaction. This is a more serious error, as it could lead to financial losses for the customer and the institution, as well as potentially damaging the institution's reputation.\n",
    "\n",
    "Given the potential financial impact and reputational risk associated with failing to detect fraudulent transactions, the financial institution would prioritize precision. A higher precision means fewer false positives, which helps to minimize the inconvenience to customers while still maintaining a high level of accuracy in detecting actual fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Q9. Provide an example of a classification problem where recall is the most important metric and explain why.__\n",
    "\n",
    "Consider a scenario in a hospital where doctors are using a model to predict whether a patient has a highly contagious and potentially life-threatening disease, like Tuberculosis (TB), based on their chest X-ray images. \n",
    "\n",
    "- **Positive Class (1)**: Patient has Tuberculosis.\n",
    "- **Negative Class (0)**: Patient does not have Tuberculosis.\n",
    "\n",
    "The model generates the following performance metrics:\n",
    "\n",
    "- **Precision**: 85%\n",
    "- **Recall**: 95%\n",
    "\n",
    "Analysis on the importance of recall:\n",
    "\n",
    "1. **False Negatives (Type II Error)**: The model incorrectly predicts that a patient does not have Tuberculosis when they actually do. This is a potentially dangerous situation, as it means a patient with an infectious disease could be mistakenly released into the community, potentially spreading the disease to others.\n",
    "\n",
    "2. **False Positives (Type I Error)**: The model incorrectly predicts that a patient has Tuberculosis when they actually don't. While this may lead to unnecessary follow-up tests and treatments, it's a less critical error compared to a false negative in this context. It's generally preferable to conduct additional tests and precautions to rule out the disease than to miss a true positive case.\n",
    "\n",
    "Given the potentially severe consequences of missing a true positive (allowing an infected patient to go undetected), the hospital would prioritize recall as the most important metric. A higher recall means the model is better at identifying true positive cases, which is critical in a medical setting to ensure the safety of both the patient and the broader community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **References:**\n",
    "- https://towardsdatascience.com/an-exhaustive-guide-to-classification-using-decision-trees-8d472e77223f\n",
    "- https://medium.com/@ramakrushna_mohapatra8594/a-step-by-step-guide-to-building-accurate-predictive-decision-tree-model-598a5cfb460d\n",
    "- https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5\n",
    "- https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n",
    "- https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html\n",
    "- https://visualstudiomagazine.com/articles/2023/02/21/scikit-decision-tree.aspx#:~:text=A%20decision%20tree%20is%20a,there%20are%20only%20two%20possibilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
